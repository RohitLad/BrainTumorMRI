{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumor Auto-Segmentation for MRI\n",
    "\n",
    "## Description\n",
    "\n",
    "Magnetic Resonance Imaging (MRI) is an imaging technique which is used to obeserve a variety of diseases and body parts. The atoms of body parts are subjected to magnetic field to which they emmit radio waves. These radio waves are then measured to get an image of the particular body part, this is how [MRI](https://en.wikipedia.org/wiki/Magnetic_resonance_imaging) works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just colab things\n",
    "onColab = True\n",
    "if onColab:\n",
    "  !rm -rf repo\n",
    "  !git clone https://github.com/RohitLad/BrainTumorMRI.git repo\n",
    "  repoDir = 'repo/'\n",
    "  %tensorflow_version 1.15.0\n",
    "else:\n",
    "  repoDir = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages andf Functions\n",
    "\n",
    "We would be using the following packages\n",
    "\n",
    "- `numpy` and `pandas` for data manipulation\n",
    "- `keras` for building Deep Learning models\n",
    "- `matplotlib` and `seaborn` for plots\n",
    "- `nibabel` to extract images and labels from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import h5py\n",
    "from functools import partial\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation, Conv3D, Deconvolution3D, MaxPooling3D, UpSampling3D\n",
    "from keras.engine import Input, Model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "K.set_image_data_format(\"channels_first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Description\n",
    "\n",
    "MR images are often encountered in [DICOM format](https://en.wikipedia.org/wiki/DICOM), these images can be processed by using the [pydicom](https://pydicom.github.io/pydicom/stable/getting_started.html) library.\n",
    "\n",
    "In this project, we shall be using data from the [Decathlon 10 Challenge](https://decathlon-10.grand-challenge.org). The data is mostly preprocessed but in real prectise, MRI data has to be significantly preprocessed in order to use it to train our models.\n",
    "\n",
    "The dataset is stored in [NifTI-1 format](https://nifti.nimh.nih.gov/nifti-1/) and we will be using the [NiBabel library](https://github.com/nipy/nibabel) to interact with the files. Every training sample consists of two files:\n",
    "\n",
    "The first file is a 3D image with 4 channels, so basically it is a 4D array of shape (240, 240, 155, 4)\n",
    "\n",
    "- The first three dimensions signify the location (X, Y and Z coordinates) for each point in the 3D volume\n",
    "- The 4th dimension is the values of four different sequences namely:\n",
    "    - FLAIR: \"Fluid Attenuated Inversion Recovery\"\n",
    "    - T1w: \"T1-weighted\"\n",
    "    - t1gd: \"T1-weighted with gadolinium contrast enhancement\"\n",
    "    - T2w: \"T2-weighted\"\n",
    "    \n",
    "The second file is a label file containing of a 3D array of shape (240, 240, 155). The labels are integers corresponding:\n",
    "   - 0: background\n",
    "   - 1: edema\n",
    "   - 2: non-enhancing tumor\n",
    "   - 3: enhancing tumor\n",
    "\n",
    "We have 484 training examples which are then split into 80% training and 20% validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the paths\n",
    "DataDir = \"Data/\"\n",
    "trainDir = DataDir+'train/'\n",
    "validDir = DataDir+'valid/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dice Similarity Coefficient\n",
    "\n",
    "A natural candidate that one comes up with as a loss function is the cross-entropy loss function. But this function is not suitable for segmentation tasks since there is a heavy class imbalance as there aren't many positive regions in an MR image dataset.\n",
    "\n",
    "A much more common loss for segmentation tasks is the Dice similarity coefficient (DSC). A DSC measures how similar two regions are or in general, how well two contours overlap.\n",
    "\n",
    "- Dice index ranges from 0 (complete mismatch) to 1 (perfect match)\n",
    "\n",
    "For two sets $A$ and $B$, the DSC is defined as:\n",
    "$$\\text{DSC}(A, B) = 2 \\times \\frac{|A \\cap B|}{|A| + |B|}$$\n",
    "Here, we can interpret $A$ as the prediction for voxel being considered and $B$ being the ground truth.\n",
    "\n",
    "Considering:\n",
    "- $x$: the input image\n",
    "- $f(x)$: the model output (prediction)\n",
    "- $y$ the label (ground truth)\n",
    "\n",
    "$$\\text{DSC}(f, x, y) = 2 \\times\\frac{ \\sum_{i, j} f(x)_{ij} \\times y_{ij} + \\epsilon}{\\sum_{i,j} f(x)_{ij} + \\sum_{i, j} y_{ij} + \\epsilon}$$\n",
    "\n",
    "- $\\epsilon$ is a small number to avoid division by zero\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/328671987/figure/fig4/AS:688210103529478@1541093483784/Calculation-of-the-Dice-similarity-coefficient-The-deformed-contour-of-the-liver-from.ppm\" width=\"30%\">\n",
    "\n",
    "[Image Source](https://www.researchgate.net/figure/Calculation-of-the-Dice-similarity-coefficient-The-deformed-contour-of-the-liver-from_fig4_328671987)\n",
    "\n",
    "\n",
    "The above is a formulation for a single class. Given $\\text{DSC}(f, x, y)$ for a single class, we can think about a multiple class approach.\n",
    "\n",
    "Consider $\\text{DSC}_i(f, x, y)$ be the Dice Coefficient for $i^{th}$ class, we can take an average over $N$ classes and therefore say\n",
    "\n",
    "$$DC(f, x, y) = \\frac{1}{N} \\sum_{i=1}^{L} \\left ( DSC_{i}(f, x, y) \\right )$$\n",
    "\n",
    "Since we want segmentations for each of three classes of conditions: edema, enhancing tumor, non-enhancing tumor, $L$ would be 3.\n",
    "\n",
    "## 4. Soft Dice Loss\n",
    "\n",
    "Since the Dice Similarity Coefficient takes in discrete values, we need an analogous formulation which takes in real valued input. This is where *Soft Dice Loss* comes in:\n",
    "\n",
    "Considering: \n",
    "- $p$: the predictions\n",
    "- $q$: the ground truth (wither 0 or 1) \n",
    "- $\\epsilon$ is a small number to avoid division by zero\n",
    "\n",
    "the *Soft Dice Loss* ${L}_{Dice}$) is given by\n",
    "$$\\mathcal{L}_{Dice}(p, q) = 1 - 2\\times\\frac{\\sum_{i, j} p_{ij}q_{ij} + \\epsilon}{\\left(\\sum_{i, j} p_{ij}^2 \\right) + \\left(\\sum_{i, j} q_{ij}^2 \\right) + \\epsilon}$$\n",
    "\n",
    "and as it is understood, for multiple classes\n",
    "\n",
    "$$\\mathcal{L}_{Dice}(p, q) = 1 - \\frac{1}{N} \\sum_{c=1}^{L} 2\\times\\frac{\\sum_{i, j} p_{cij}q_{cij} + \\epsilon}{\\left(\\sum_{i, j} p_{cij}^2 \\right) + \\left(\\sum_{i, j} q_{cij}^2 \\right) + \\epsilon}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diceCoefficient(yTrue, yPred, axis=(1,2,3), eps=1e-5):\n",
    "    \"\"\"\n",
    "    Calculate the dice coefficient over all classes\n",
    "    \n",
    "    yTrue: tensor of ground truth values (numClasses, yDim, yDim, zDim)\n",
    "    yPred: tensor of soft predictions (numClasses, yDim, yDim, zDim)\n",
    "    axis: spatial axes to sum over while computing Numerator and Denominator\n",
    "    eps: small constant to avoid dividing by zero\n",
    "    \n",
    "    returns:\n",
    "    diceCoefficient: computed value of soft dice coefficient\n",
    "    \"\"\"\n",
    "    numerator = 2*K.sum(yTrue*yPred, axis=axis)+eps\n",
    "    denominator = K.sum(yPred**2, axis=axis)+K.sum(yTrue**2, axis=axis)+eps\n",
    "    return K.mean(numerator/denominator)\n",
    "\n",
    "def softDiceLoss(yTrue, yPred, axis=(1,2,3), eps=1e-5):\n",
    "    \"\"\"\n",
    "    Calculate the soft dice loss over all classes\n",
    "    \n",
    "    yTrue: tensor of ground truth values (numClasses, yDim, yDim, zDim)\n",
    "    yPred: tensor of soft predictions (numClasses, yDim, yDim, zDim)\n",
    "    axis: spatial axes to sum over while computing Numerator and Denominator\n",
    "    eps: small constant to avoid dividing by zero\n",
    "    \n",
    "    returns:\n",
    "    diceLoss: computed value of soft dice loss\n",
    "    \"\"\"\n",
    "    \n",
    "    numerator = 2*K.sum(yTrue*yPred, axis=axis)+eps\n",
    "    denominator = K.sum(yPred**2, axis=axis)+K.sum(yTrue**2, axis=axis)+eps\n",
    "    return 1. - K.mean(numerator/denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model: 3D U-Net\n",
    "\n",
    "A [3D U-net](https://arxiv.org/abs/1606.06650) is used for this task which takes advantage of the volumetric shape of MR images and is one of the best performing models for this task. Know more about the architecture from [this paper](https://arxiv.org/abs/1606.06650).\n",
    "\n",
    "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "Let us create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet3D:\n",
    "    \n",
    "    def __init__(self, optimizer, lossFunction, inputShape=(4,160,160,16), poolSize=(2,2,2), nLabels=3, lr=1e-5, depth=4, nBaseFilters=32, activation='sigmoid', metrics=[], batchNormalization=False, deconvolution=False):\n",
    "        self.optimizer = optimizer\n",
    "        self.lossFunction = lossFunction\n",
    "        self.inputShape = inputShape\n",
    "        self.poolSize = poolSize\n",
    "        self.nLabels = nLabels\n",
    "        self.learningRate = lr\n",
    "        self.depth = depth\n",
    "        self.nBaseFilters = nBaseFilters\n",
    "        self.activation = activation\n",
    "        self.metrics = [metrics] if not isinstance(metrics, list) else metrics\n",
    "        self.deconvolution = deconvolution\n",
    "        self.batchNormalization = batchNormalization\n",
    "        self.levels = []\n",
    "        self.model = None\n",
    "    \n",
    "    def _createConvBlock(self, inputLayer, nFilters, batchNorm=False, instanceNorm=False, kernel=(3,3,3), activation='relu', padding='same', strides=(1,1,1)):\n",
    "        layer = Conv3D(nFilters, kernel, padding=padding, strides=strides)(inputLayer)\n",
    "        return Activation(activation)(layer)\n",
    "    \n",
    "    def _createUpConvBlock(self, nFilters, poolSize, kernelSize=(2,2,2), strides=(2,2,2), deconvolution=False):\n",
    "        if deconvolution:\n",
    "            return Deconvolution3D(filters=nFilters, kernel_size=kernelSize, strides=strides)\n",
    "        else:\n",
    "            return UpSampling3D(size=poolSize)\n",
    "    \n",
    "    def _getCurrFilters(self, depth):\n",
    "        return self.nBaseFilters*(2**depth)\n",
    "    \n",
    "    def createModel(self):\n",
    "        inputs = Input(self.inputShape)\n",
    "        currLayer = inputs\n",
    "        self.levels = []\n",
    "        self.model = None\n",
    "        \n",
    "        for d in range(self.depth):\n",
    "            currFilters = self._getCurrFilters(depth=d)\n",
    "            layer1 = self._createConvBlock(inputLayer=currLayer, nFilters=currFilters, batchNorm=self.batchNormalization)\n",
    "            layer2 = self._createConvBlock(inputLayer=layer1, nFilters=currFilters*2, batchNorm=self.batchNormalization)\n",
    "            if d < self.depth-1:\n",
    "                currLayer = MaxPooling3D(pool_size=self.poolSize)(layer2)\n",
    "                self.levels.append([layer1, layer2, currLayer])\n",
    "            else:\n",
    "                currLayer = layer2\n",
    "                self.levels.append([layer1, layer2])                \n",
    "\n",
    "        for d in reversed(range(self.depth-1)):\n",
    "            upConv = self._createUpConvBlock(nFilters=currLayer._keras_shape[-1], poolSize=self.poolSize, deconvolution=self.deconvolution)(currLayer)\n",
    "            concat = concatenate([upConv, self.levels[d][1]], axis=1)\n",
    "            currFilters = self.levels[d][1]._keras_shape[1]\n",
    "            layer1 = self._createConvBlock(inputLayer=concat, nFilters=currFilters, batchNorm=self.batchNormalization)\n",
    "            currLayer = self._createConvBlock(inputLayer=layer1, nFilters=currFilters, batchNorm=self.batchNormalization)\n",
    "        \n",
    "        finalLayer = Conv3D(self.nLabels, (1,1,1), activation=self.activation)(currLayer)\n",
    "        self.model = Model(inputs=inputs, outputs=finalLayer)\n",
    "        self.model.compile(optimizer=self.optimizer(lr=self.learningRate), loss=self.lossFunction, metrics=self.metrics)\n",
    "        \n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepDecay(epoch, initLR, drop, epochDrop):\n",
    "    return initLR*drop**np.floor((1+epoch)/float(epochDrop))\n",
    "\n",
    "def getCallbacks(modelFile, initLR=1e-4, lrDrop=0.5, lrEpochs=None, lrPatience=50, loggingFile='training.log', verbosity=1, earlystopPatience=None):\n",
    "    callbacks = list()\n",
    "    callbacks.append(ModelCheckpoint(modelFile, save_best_only=True))\n",
    "    callbacks.append(CSVLogger(loggingFile, append=True))\n",
    "    \n",
    "    if lrEpochs:\n",
    "        callbacks.append(LearningRateScheduler(partial(stepDecay, initLR=initLR, drop=lrDrop, epochDrop=lrEpochs)))\n",
    "    else:\n",
    "        callbacks.append(ReduceLROnPlateau(factor=lrDrop, patience=lrPatience, verbose=verbosity))\n",
    "    \n",
    "    if earlystopPatience:\n",
    "        callbacks.append(EarlyStopping(verbose=verbosity, patience=earlystopPatience))\n",
    "    \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, the entire preprocessed dataset is stored in the `h5py` format. We shall write a custom Keras `Sequence` class to be used as a `Generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, baseDir, listIDs, batchSize=1, dim=(160,160,16), nChannels=4, nClasses=3, shuffle=True):\n",
    "    \n",
    "        'Initialization'\n",
    "        self.baseDir = baseDir\n",
    "        self.dim = dim\n",
    "        self.batchSize = batchSize\n",
    "        self.listIDs = listIDs\n",
    "        self.nChannels = nChannels\n",
    "        self.nClasses = nClasses\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = None\n",
    "        self.on_epoch_end()\n",
    "        self.allX = {}\n",
    "        self.ally = {}\n",
    "        self.initData()\n",
    "        \n",
    "    def initData(self):\n",
    "        for ids in tempListIDs:\n",
    "            with h5py.File(self.baseDir+ids,'r') as f:\n",
    "                self.allX[ids] = np.array(f.get('x'))\n",
    "                self.ally[ids] = np.moveaxis(np.array(f.get('y')), 3, 0)[1:]\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.listIDs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __data_generation(self, tempListIDs):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, n_channels, *dim)\n",
    "        \n",
    "        # Initialization\n",
    "        X = np.zeros((self.batchSize, self.nChannels, *self.dim), dtype = np.float64)\n",
    "        y = np.zeros((self.batchSize, self.nClasses, *self.dim), dtype=np.float64)\n",
    "        \n",
    "        # Data Generation\n",
    "        for i, ids in enumerate(tempListIDs):\n",
    "            #with h5py.File(self.baseDir+ids, 'r') as f:\n",
    "            #    X[i] = np.array(f.get('x'))\n",
    "            #    y[i] = np.moveaxis(np.array(f.get('y')), 3, 0)[1:]\n",
    "            X[i] = self.allX[ids]\n",
    "            y[i] = self.ally[ids]\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Number of batches per epoch'\n",
    "        return int(np.floor(len(self.listIDs)/self.batchSize))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'generate a batch of data'\n",
    "        indexes = self.indexes[index*self.batchSize: (index+1)*self.batchSize]\n",
    "        sampleList = [self.listIDs[i] for i in indexes]\n",
    "        X,y = self.__data_generation(sampleList)\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(DataDir+'config.json') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 4, 160, 160,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_16 (Conv3D)              (None, 32, 160, 160, 3488        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 160, 160, 0           conv3d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_17 (Conv3D)              (None, 64, 160, 160, 55360       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 64, 160, 160, 0           conv3d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3D)  (None, 64, 80, 80, 8 0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_18 (Conv3D)              (None, 64, 80, 80, 8 110656      max_pooling3d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 64, 80, 80, 8 0           conv3d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_19 (Conv3D)              (None, 128, 80, 80,  221312      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 128, 80, 80,  0           conv3d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3D)  (None, 128, 40, 40,  0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_20 (Conv3D)              (None, 128, 40, 40,  442496      max_pooling3d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 128, 40, 40,  0           conv3d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_21 (Conv3D)              (None, 256, 40, 40,  884992      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 256, 40, 40,  0           conv3d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3D)  (None, 256, 20, 20,  0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_22 (Conv3D)              (None, 256, 20, 20,  1769728     max_pooling3d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 256, 20, 20,  0           conv3d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_23 (Conv3D)              (None, 512, 20, 20,  3539456     activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 512, 20, 20,  0           conv3d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_4 (UpSampling3D)  (None, 512, 40, 40,  0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 768, 40, 40,  0           up_sampling3d_4[0][0]            \n",
      "                                                                 activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_24 (Conv3D)              (None, 256, 40, 40,  5308672     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 256, 40, 40,  0           conv3d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_25 (Conv3D)              (None, 256, 40, 40,  1769728     activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 256, 40, 40,  0           conv3d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_5 (UpSampling3D)  (None, 256, 80, 80,  0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 384, 80, 80,  0           up_sampling3d_5[0][0]            \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_26 (Conv3D)              (None, 128, 80, 80,  1327232     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 128, 80, 80,  0           conv3d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_27 (Conv3D)              (None, 128, 80, 80,  442496      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 128, 80, 80,  0           conv3d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_6 (UpSampling3D)  (None, 128, 160, 160 0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 192, 160, 160 0           up_sampling3d_6[0][0]            \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_28 (Conv3D)              (None, 64, 160, 160, 331840      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 64, 160, 160, 0           conv3d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_29 (Conv3D)              (None, 64, 160, 160, 110656      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 64, 160, 160, 0           conv3d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_30 (Conv3D)              (None, 3, 160, 160,  195         activation_28[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 16,318,307\n",
      "Trainable params: 16,318,307\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trainGen = DataGenerator(baseDir=trainDir, listIDs=config['train'], batchSize=3, dim=(160,160,16))\n",
    "validGen = DataGenerator(baseDir=validDir, listIDs=config['valid'], batchSize=3, dim=(160,160,16))\n",
    "model = UNet3D(optimizer=Adam, lossFunction=softDiceLoss, metrics=[diceCoefficient]).createModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, modelFile, trainGen, validGen, stepsPerEpoch, validSteps, initLR=1e-3, \n",
    "               lrDrop=0.5, lrEpochs=None, nEpochs=500, lrPatience=20, earlyStopPatience=None):\n",
    "    \n",
    "    callbacks = getCallbacks(modelFile=modelFile, initLR=initLR, lrDrop=lrDrop,\n",
    "                            lrEpochs=lrEpochs, lrPatience=lrPatience, earlystopPatience=earlyStopPatience)\n",
    "    if onColab:\n",
    "        mp = True\n",
    "        wrks = 16\n",
    "    else:\n",
    "        mp = False\n",
    "        wrks = 1\n",
    "    model.fit_generator(generator=trainGen,\n",
    "                        steps_per_epoch=stepsPerEpoch,\n",
    "                        epochs=nEpochs,\n",
    "                        validation_data=validGen,\n",
    "                        validation_steps=validSteps,\n",
    "                        callbacks=callbacks,\n",
    "                        use_multiprocessing=mp,\n",
    "                        workers=wrks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-accdc053e40e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelFile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MRISegmentation.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainGen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainGen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidGen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidGen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstepsPerEpoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidSteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-46cd8a3c7a90>\u001b[0m in \u001b[0;36mtrainModel\u001b[1;34m(model, modelFile, trainGen, validGen, stepsPerEpoch, validSteps, initLR, lrDrop, lrEpochs, nEpochs, lrPatience, earlyStopPatience)\u001b[0m\n\u001b[0;32m     10\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidGen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidSteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                         callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\rohit\\desktop\\coursera\\braintumormri\\venv\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rohit\\desktop\\coursera\\braintumormri\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rohit\\desktop\\coursera\\braintumormri\\venv\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                                             reset_metrics=False)\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rohit\\desktop\\coursera\\braintumormri\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rohit\\desktop\\coursera\\braintumormri\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mc:\\users\\rohit\\desktop\\coursera\\braintumormri\\venv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainModel(model, modelFile='MRISegmentation.h5', trainGen=trainGen, validGen=validGen, stepsPerEpoch=100, validSteps=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
